{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1b7RV-22-Hanw1tMl0w9l_6Qa6CXeumvc","authorship_tag":"ABX9TyMWez16PhduP35E18jnqPRw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","!cp /content/drive/MyDrive/2-folder/kaggle/df_utils.py /content/\n","import df_utils"],"metadata":{"id":"N38m0isXjjUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1 - Preparing the Data:\n","# The first step is to prepare the data for modeling.\n","# This entails identifying the relevant features, cleaning the data,\n","# and dividing it into training and validation sets."],"metadata":{"id":"_ZRtPpAxX0P_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_loaded = pd.read_csv('/content/drive/MyDrive/2-folder/kaggle/housing-prices-competition/train.csv')\n","df_test_loaded = pd.read_csv('/content/drive/MyDrive/2-folder/kaggle/housing-prices-competition/test.csv')\n","\n","# num_col = len(df_train.columns)\n","# print(f\"num_col = {num_col}\")\n","# print(df_train[\"SalePrice\"][:5])\n","\n","df_train = df_train_loaded.drop(['Id'], axis=1)\n","df_test = df_test_loaded.drop(['Id'], axis=1)\n","\n","# Drop all columns with with more than 20% of missing values\n","percent_missing = df_train.isnull().sum() * 100 / len(df_train)\n","missing_value_df = pd.DataFrame({'column_name': df_train.columns,\n","                                 'percent_missing': percent_missing})\n","missing_value_df = missing_value_df.loc[missing_value_df['percent_missing'] >= 20]\n","# print(missing_value_df)\n","df_train = df_train.drop(['Alley', 'MasVnrType', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n","df_test = df_test.drop(['Alley', 'MasVnrType', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n","\n","# Select only numeric types\n","# print(df_train.dtypes)\n","df_train = df_train.select_dtypes(include=['int64', 'float64'])\n","\n","# Calculate the correlation of features with the target\n","correlation = df_train.corr()\n","sorted_corr = correlation['SalePrice'].sort_values(ascending=False)\n","# print(sorted_corr)\n","columns = []\n","for i, v in sorted_corr.items():\n","  if v > 0.3 and i != 'SalePrice':\n","    print('index: ', i, 'value: ', v)\n","    columns.append(i)\n","# print(columns)\n","\n","# Remove target label from training set\n","y_train = df_train['SalePrice'].values\n","# print(y[0:5])\n","df_train = df_train.drop(['SalePrice'], axis=1)\n","\n","# Extract columns with high correlaton\n","df_train = df_train[columns]\n","df_test = df_test[columns]\n","print(df_train.head(5))\n","\n","# Fill missing values for numerical columns with the median\n","df_train.fillna(df_train.mean(), inplace=True)\n","df_test.fillna(df_test.mean(), inplace=True)\n","print(y_train.shape)\n","print(df_train.shape)\n","print(df_test.shape)\n","\n","# create X and y for training\n","X = df_train.values\n","y = y_train\n","\n","# create train, validation and test splits\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ls_Hf3IsjiuH","executionInfo":{"status":"ok","timestamp":1735485076642,"user_tz":-60,"elapsed":773,"user":{"displayName":"L","userId":"12971862049250805533"}},"outputId":"efe89632-5056-4170-df39-e81557c69190"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["index:  OverallQual value:  0.7909816005838053\n","index:  GrLivArea value:  0.7086244776126515\n","index:  GarageCars value:  0.6404091972583519\n","index:  GarageArea value:  0.6234314389183622\n","index:  TotalBsmtSF value:  0.6135805515591943\n","index:  1stFlrSF value:  0.6058521846919153\n","index:  FullBath value:  0.5606637627484453\n","index:  TotRmsAbvGrd value:  0.5337231555820284\n","index:  YearBuilt value:  0.5228973328794967\n","index:  YearRemodAdd value:  0.5071009671113866\n","index:  GarageYrBlt value:  0.4863616774878596\n","index:  MasVnrArea value:  0.47749304709571444\n","index:  Fireplaces value:  0.46692883675152763\n","index:  BsmtFinSF1 value:  0.3864198062421535\n","index:  LotFrontage value:  0.35179909657067737\n","index:  WoodDeckSF value:  0.32441344456812926\n","index:  2ndFlrSF value:  0.31933380283206736\n","index:  OpenPorchSF value:  0.31585622711605504\n","   OverallQual  GrLivArea  GarageCars  GarageArea  TotalBsmtSF  1stFlrSF  \\\n","0            7       1710           2         548          856       856   \n","1            6       1262           2         460         1262      1262   \n","2            7       1786           2         608          920       920   \n","3            7       1717           3         642          756       961   \n","4            8       2198           3         836         1145      1145   \n","\n","   FullBath  TotRmsAbvGrd  YearBuilt  YearRemodAdd  GarageYrBlt  MasVnrArea  \\\n","0         2             8       2003          2003       2003.0       196.0   \n","1         2             6       1976          1976       1976.0         0.0   \n","2         2             6       2001          2002       2001.0       162.0   \n","3         1             7       1915          1970       1998.0         0.0   \n","4         2             9       2000          2000       2000.0       350.0   \n","\n","   Fireplaces  BsmtFinSF1  LotFrontage  WoodDeckSF  2ndFlrSF  OpenPorchSF  \n","0           0         706         65.0           0       854           61  \n","1           1         978         80.0         298         0            0  \n","2           1         486         68.0           0       866           42  \n","3           1         216         60.0           0       756           35  \n","4           1         655         84.0         192      1053           84  \n","(1460,)\n","(1460, 18)\n","(1459, 18)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvNQqjRUWG0F"},"outputs":[],"source":["# 2 - Model Selection:\n","# The following step is to choose the base models\n","# that will be used in the stacking ensemble.\n","# A broad selection of models is typically chosen to guarantee\n","# that they produce different types of errors and complement one another."]},{"cell_type":"code","source":["# 3 - Training the Base Models:\n","# After selecting the base models, they are trained on the training set.\n","# To ensure diversity, each model is trained using a different algorithm\n","# or set of hyperparameters."],"metadata":{"id":"Vg2_XrC9aK6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importa le librerie necessarie\n","import pandas as pd\n","import numpy as np\n","from xgboost import XGBRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# # Carica i dati\n","# train = pd.read_csv(\"train.csv\")\n","# test = pd.read_csv(\"test.csv\")\n","\n","# # Salva gli ID per la sottomissione\n","# test_ids = test[\"Id\"]\n","\n","# # Rimuovi la colonna 'Id' dai dati\n","# train.drop(\"Id\", axis=1, inplace=True)\n","# test.drop(\"Id\", axis=1, inplace=True)\n","\n","# # Separazione delle feature e del target\n","# X = train.drop(\"SalePrice\", axis=1)\n","# y = train[\"SalePrice\"]\n","\n","# # Identifica colonne numeriche e categoriche\n","# numerical_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n","# categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n","\n","# # Preprocessing dei dati numerici: sostituzione dei valori mancanti\n","# num_imputer = SimpleImputer(strategy=\"median\")\n","# X[numerical_cols] = num_imputer.fit_transform(X[numerical_cols])\n","# test[numerical_cols] = num_imputer.transform(test[numerical_cols])\n","\n","# # Preprocessing dei dati categorici: sostituzione dei valori mancanti e codifica one-hot\n","# cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n","# X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n","# test[categorical_cols] = cat_imputer.transform(test[categorical_cols])\n","\n","# encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n","# X_encoded = pd.DataFrame(encoder.fit_transform(X[categorical_cols]))\n","# test_encoded = pd.DataFrame(encoder.transform(test[categorical_cols]))\n","\n","# # Mantieni gli indici originali per l'unione\n","# X_encoded.index = X.index\n","# test_encoded.index = test.index\n","\n","# # Rimuovi le colonne categoriche originali e aggiungi quelle codificate\n","# X = X.drop(categorical_cols, axis=1)\n","# test = test.drop(categorical_cols, axis=1)\n","# X = pd.concat([X, X_encoded], axis=1)\n","# test = pd.concat([test, test_encoded], axis=1)\n","\n","# Dividi i dati di addestramento e validazione\n","# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Inizializza e addestra il modello XGBoost\n","model_0 = XGBRegressor(\n","    n_estimators=500,\n","    learning_rate=0.05,\n","    max_depth=4,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    random_state=42\n",")\n","\n","model_0.fit(X_train, y_train)\n","\n","# Valuta il modello\n","y_pred = model_0.predict(X_val)\n","rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n","print(f\"Validation RMSE: {rmse:.2f}\")\n","\n","print(type(y_pred))\n","print(y_pred.shape)\n","print(y_pred[:5])\n","print(y_train[:5])\n","y_pred = np.expand_dims(y_pred, axis=1)\n","print(y_pred[:5])\n","print(y_pred.shape)\n","\n","model0_pred = y_pred\n","\n","# Addestra il modello su tutti i dati e fai predizioni sul set di test\n","# model.fit(X, y)\n","# predictions = model.predict(test)\n","\n","# Crea il file di sottomissione\n","# submission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": predictions})\n","# submission.to_csv(\"submission.csv\", index=False)\n","# print(\"File di sottomissione creato: submission.csv\")\n"],"metadata":{"id":"Ox2hzbK-WcSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735485077968,"user_tz":-60,"elapsed":1333,"user":{"displayName":"L","userId":"12971862049250805533"}},"outputId":"4d8d6982-927b-41af-9eaa-60b3920b358b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation RMSE: 28161.58\n","<class 'numpy.ndarray'>\n","(146,)\n","[142370.94 312801.12 114955.88 162205.28 317470.34]\n","[250000 187100 133900  67000 137500]\n","[[142370.94]\n"," [312801.12]\n"," [114955.88]\n"," [162205.28]\n"," [317470.34]]\n","(146, 1)\n"]}]},{"cell_type":"code","source":["# Convert data to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","X_valid_tensor = torch.tensor(X_val, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n","y_valid_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n","# test_tensor = torch.tensor(test.values, dtype=torch.float32)\n","\n","# Define the model\n","class HousePriceModel(nn.Module):\n","    def __init__(self, input_size):\n","        super(HousePriceModel, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(input_size, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","# Initialize the model\n","input_size = X_train.shape[1]\n","model_1 = HousePriceModel(input_size)\n","\n","# Define loss and optimizer\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model_1.parameters(), lr=0.001)\n","\n","# Training loop\n","epochs = 3000\n","for epoch in range(epochs):\n","    model_1.train()\n","    optimizer.zero_grad()\n","    predictions = model_1(X_train_tensor)\n","    loss = criterion(predictions, y_train_tensor)\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Validation\n","    model_1.eval()\n","    with torch.no_grad():\n","        val_predictions = model_1(X_valid_tensor)\n","        val_loss = criterion(val_predictions, y_valid_tensor)\n","\n","    if epoch % 20 == 0:\n","        print(f\"Epoch {epoch}/{epochs}, Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n","\n","# Evaluate on validation set\n","model_1.eval()\n","with torch.no_grad():\n","    val_predictions = model_1(X_valid_tensor)\n","    rmse = np.sqrt(mean_squared_error(y_val, val_predictions.numpy()))\n","print(f\"Validation RMSE: {rmse:.2f}\")\n","\n","val_predictions = val_predictions.numpy()\n","print(type(val_predictions))\n","print(val_predictions.shape)\n","print(val_predictions[:5])\n","print(y_train[:5])\n","# y_pred = np.expand_dims(val_predictions, axis=1)\n","# print(val_predictions[:5])\n","model1_pred = val_predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqw71dNBgxij","executionInfo":{"status":"ok","timestamp":1735485101080,"user_tz":-60,"elapsed":23118,"user":{"displayName":"L","userId":"12971862049250805533"}},"outputId":"548906dc-d98f-4994-e41b-564573986387"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/3000, Train Loss: 38580862976.0000, Validation Loss: 42474012672.0000\n","Epoch 20/3000, Train Loss: 37700169728.0000, Validation Loss: 41515839488.0000\n","Epoch 40/3000, Train Loss: 34902269952.0000, Validation Loss: 38484254720.0000\n","Epoch 60/3000, Train Loss: 28025366528.0000, Validation Loss: 31052630016.0000\n","Epoch 80/3000, Train Loss: 16556163072.0000, Validation Loss: 18717530112.0000\n","Epoch 100/3000, Train Loss: 5928761856.0000, Validation Loss: 7334825472.0000\n","Epoch 120/3000, Train Loss: 3459739904.0000, Validation Loss: 4317097984.0000\n","Epoch 140/3000, Train Loss: 3485360896.0000, Validation Loss: 4232840192.0000\n","Epoch 160/3000, Train Loss: 3285896448.0000, Validation Loss: 4248665088.0000\n","Epoch 180/3000, Train Loss: 3323302144.0000, Validation Loss: 4127312640.0000\n","Epoch 200/3000, Train Loss: 3461856000.0000, Validation Loss: 4013596928.0000\n","Epoch 220/3000, Train Loss: 3275376384.0000, Validation Loss: 3914719744.0000\n","Epoch 240/3000, Train Loss: 3301137152.0000, Validation Loss: 3807156736.0000\n","Epoch 260/3000, Train Loss: 3245720576.0000, Validation Loss: 3694980608.0000\n","Epoch 280/3000, Train Loss: 3055898880.0000, Validation Loss: 3584904448.0000\n","Epoch 300/3000, Train Loss: 3180599040.0000, Validation Loss: 3474526464.0000\n","Epoch 320/3000, Train Loss: 2953527808.0000, Validation Loss: 3372083712.0000\n","Epoch 340/3000, Train Loss: 2735061248.0000, Validation Loss: 3260061184.0000\n","Epoch 360/3000, Train Loss: 2942810368.0000, Validation Loss: 3163327232.0000\n","Epoch 380/3000, Train Loss: 2772369408.0000, Validation Loss: 3057102336.0000\n","Epoch 400/3000, Train Loss: 2738370304.0000, Validation Loss: 2962915584.0000\n","Epoch 420/3000, Train Loss: 2829965312.0000, Validation Loss: 2884381696.0000\n","Epoch 440/3000, Train Loss: 2660716544.0000, Validation Loss: 2782734336.0000\n","Epoch 460/3000, Train Loss: 2609930752.0000, Validation Loss: 2703574784.0000\n","Epoch 480/3000, Train Loss: 2583665664.0000, Validation Loss: 2631654400.0000\n","Epoch 500/3000, Train Loss: 2646436608.0000, Validation Loss: 2545701888.0000\n","Epoch 520/3000, Train Loss: 2440556544.0000, Validation Loss: 2485906176.0000\n","Epoch 540/3000, Train Loss: 2493233408.0000, Validation Loss: 2421831936.0000\n","Epoch 560/3000, Train Loss: 2639809536.0000, Validation Loss: 2367262976.0000\n","Epoch 580/3000, Train Loss: 2548556032.0000, Validation Loss: 2313718528.0000\n","Epoch 600/3000, Train Loss: 2400633088.0000, Validation Loss: 2269403904.0000\n","Epoch 620/3000, Train Loss: 2578227712.0000, Validation Loss: 2243142656.0000\n","Epoch 640/3000, Train Loss: 2563157760.0000, Validation Loss: 2192526080.0000\n","Epoch 660/3000, Train Loss: 2375668736.0000, Validation Loss: 2171744512.0000\n","Epoch 680/3000, Train Loss: 2733493248.0000, Validation Loss: 2132686464.0000\n","Epoch 700/3000, Train Loss: 2347973120.0000, Validation Loss: 2106690176.0000\n","Epoch 720/3000, Train Loss: 2317760768.0000, Validation Loss: 2099393664.0000\n","Epoch 740/3000, Train Loss: 2447431168.0000, Validation Loss: 2057920896.0000\n","Epoch 760/3000, Train Loss: 2337036544.0000, Validation Loss: 2029995136.0000\n","Epoch 780/3000, Train Loss: 2546998272.0000, Validation Loss: 2014810496.0000\n","Epoch 800/3000, Train Loss: 2438823168.0000, Validation Loss: 2001749760.0000\n","Epoch 820/3000, Train Loss: 2300219904.0000, Validation Loss: 1997969792.0000\n","Epoch 840/3000, Train Loss: 2289503744.0000, Validation Loss: 1982102144.0000\n","Epoch 860/3000, Train Loss: 2278697984.0000, Validation Loss: 1989639936.0000\n","Epoch 880/3000, Train Loss: 2477981696.0000, Validation Loss: 1964874368.0000\n","Epoch 900/3000, Train Loss: 2220394496.0000, Validation Loss: 1965639168.0000\n","Epoch 920/3000, Train Loss: 2512029952.0000, Validation Loss: 1958605568.0000\n","Epoch 940/3000, Train Loss: 2304478976.0000, Validation Loss: 1960781184.0000\n","Epoch 960/3000, Train Loss: 2505351680.0000, Validation Loss: 1960140288.0000\n","Epoch 980/3000, Train Loss: 2400614656.0000, Validation Loss: 1968146176.0000\n","Epoch 1000/3000, Train Loss: 2307744512.0000, Validation Loss: 1965239296.0000\n","Epoch 1020/3000, Train Loss: 2457713408.0000, Validation Loss: 1977286144.0000\n","Epoch 1040/3000, Train Loss: 2405985536.0000, Validation Loss: 1955408896.0000\n","Epoch 1060/3000, Train Loss: 2415582976.0000, Validation Loss: 1969326720.0000\n","Epoch 1080/3000, Train Loss: 2314859520.0000, Validation Loss: 1964345088.0000\n","Epoch 1100/3000, Train Loss: 2367783168.0000, Validation Loss: 1968368384.0000\n","Epoch 1120/3000, Train Loss: 2392894976.0000, Validation Loss: 1959759104.0000\n","Epoch 1140/3000, Train Loss: 2389792000.0000, Validation Loss: 1975471872.0000\n","Epoch 1160/3000, Train Loss: 2211666176.0000, Validation Loss: 1975737344.0000\n","Epoch 1180/3000, Train Loss: 2327322112.0000, Validation Loss: 1971037824.0000\n","Epoch 1200/3000, Train Loss: 2319337216.0000, Validation Loss: 1976572928.0000\n","Epoch 1220/3000, Train Loss: 2188852992.0000, Validation Loss: 1976271104.0000\n","Epoch 1240/3000, Train Loss: 2119991808.0000, Validation Loss: 1964430208.0000\n","Epoch 1260/3000, Train Loss: 2299299584.0000, Validation Loss: 1966717824.0000\n","Epoch 1280/3000, Train Loss: 2195883008.0000, Validation Loss: 1960410240.0000\n","Epoch 1300/3000, Train Loss: 2247239168.0000, Validation Loss: 1965505664.0000\n","Epoch 1320/3000, Train Loss: 2207449856.0000, Validation Loss: 1967148800.0000\n","Epoch 1340/3000, Train Loss: 2225338112.0000, Validation Loss: 1974606208.0000\n","Epoch 1360/3000, Train Loss: 2266988288.0000, Validation Loss: 1969702016.0000\n","Epoch 1380/3000, Train Loss: 2327108608.0000, Validation Loss: 1963947648.0000\n","Epoch 1400/3000, Train Loss: 2394228480.0000, Validation Loss: 1966209664.0000\n","Epoch 1420/3000, Train Loss: 2241894144.0000, Validation Loss: 1970693760.0000\n","Epoch 1440/3000, Train Loss: 2411936000.0000, Validation Loss: 1978666752.0000\n","Epoch 1460/3000, Train Loss: 2322341376.0000, Validation Loss: 1969586432.0000\n","Epoch 1480/3000, Train Loss: 2315021568.0000, Validation Loss: 1974188544.0000\n","Epoch 1500/3000, Train Loss: 2428844032.0000, Validation Loss: 1986390144.0000\n","Epoch 1520/3000, Train Loss: 2234189568.0000, Validation Loss: 1977651328.0000\n","Epoch 1540/3000, Train Loss: 2370089472.0000, Validation Loss: 1978769920.0000\n","Epoch 1560/3000, Train Loss: 2371849472.0000, Validation Loss: 1980332032.0000\n","Epoch 1580/3000, Train Loss: 2348945408.0000, Validation Loss: 1967244416.0000\n","Epoch 1600/3000, Train Loss: 2300401664.0000, Validation Loss: 1970615168.0000\n","Epoch 1620/3000, Train Loss: 2369288704.0000, Validation Loss: 1976497920.0000\n","Epoch 1640/3000, Train Loss: 2263918848.0000, Validation Loss: 1978141824.0000\n","Epoch 1660/3000, Train Loss: 2162715648.0000, Validation Loss: 1981179136.0000\n","Epoch 1680/3000, Train Loss: 2231305472.0000, Validation Loss: 1983443456.0000\n","Epoch 1700/3000, Train Loss: 2487962880.0000, Validation Loss: 1972538624.0000\n","Epoch 1720/3000, Train Loss: 2251342336.0000, Validation Loss: 1971838592.0000\n","Epoch 1740/3000, Train Loss: 2351716096.0000, Validation Loss: 2000314496.0000\n","Epoch 1760/3000, Train Loss: 2178794496.0000, Validation Loss: 1982094848.0000\n","Epoch 1780/3000, Train Loss: 2241673728.0000, Validation Loss: 1983815168.0000\n","Epoch 1800/3000, Train Loss: 2149147392.0000, Validation Loss: 1974833152.0000\n","Epoch 1820/3000, Train Loss: 2185493760.0000, Validation Loss: 1994953600.0000\n","Epoch 1840/3000, Train Loss: 2224456960.0000, Validation Loss: 1986321664.0000\n","Epoch 1860/3000, Train Loss: 2542551808.0000, Validation Loss: 1991144832.0000\n","Epoch 1880/3000, Train Loss: 2119487360.0000, Validation Loss: 2004463488.0000\n","Epoch 1900/3000, Train Loss: 2420148224.0000, Validation Loss: 1990421632.0000\n","Epoch 1920/3000, Train Loss: 2294424576.0000, Validation Loss: 1988643072.0000\n","Epoch 1940/3000, Train Loss: 2514219008.0000, Validation Loss: 1985756672.0000\n","Epoch 1960/3000, Train Loss: 2559412480.0000, Validation Loss: 1989729280.0000\n","Epoch 1980/3000, Train Loss: 2408536832.0000, Validation Loss: 1984055296.0000\n","Epoch 2000/3000, Train Loss: 2183782912.0000, Validation Loss: 1987500800.0000\n","Epoch 2020/3000, Train Loss: 2222679552.0000, Validation Loss: 1991940480.0000\n","Epoch 2040/3000, Train Loss: 2357657344.0000, Validation Loss: 2008063872.0000\n","Epoch 2060/3000, Train Loss: 2236579328.0000, Validation Loss: 1986714240.0000\n","Epoch 2080/3000, Train Loss: 2251687680.0000, Validation Loss: 1988008320.0000\n","Epoch 2100/3000, Train Loss: 2393149440.0000, Validation Loss: 1997828992.0000\n","Epoch 2120/3000, Train Loss: 2304549376.0000, Validation Loss: 1999652352.0000\n","Epoch 2140/3000, Train Loss: 2189687040.0000, Validation Loss: 2006877312.0000\n","Epoch 2160/3000, Train Loss: 2258201088.0000, Validation Loss: 2006761216.0000\n","Epoch 2180/3000, Train Loss: 2271553536.0000, Validation Loss: 1999076480.0000\n","Epoch 2200/3000, Train Loss: 2277438720.0000, Validation Loss: 2003402240.0000\n","Epoch 2220/3000, Train Loss: 2061544320.0000, Validation Loss: 1998830080.0000\n","Epoch 2240/3000, Train Loss: 2153907200.0000, Validation Loss: 2001963648.0000\n","Epoch 2260/3000, Train Loss: 2246545408.0000, Validation Loss: 1978014976.0000\n","Epoch 2280/3000, Train Loss: 2306311680.0000, Validation Loss: 1979342080.0000\n","Epoch 2300/3000, Train Loss: 2291330048.0000, Validation Loss: 1985941504.0000\n","Epoch 2320/3000, Train Loss: 2331943680.0000, Validation Loss: 1988885376.0000\n","Epoch 2340/3000, Train Loss: 2127744896.0000, Validation Loss: 2007076864.0000\n","Epoch 2360/3000, Train Loss: 2093808384.0000, Validation Loss: 1985156608.0000\n","Epoch 2380/3000, Train Loss: 2305982720.0000, Validation Loss: 1977431168.0000\n","Epoch 2400/3000, Train Loss: 2461967360.0000, Validation Loss: 1990481152.0000\n","Epoch 2420/3000, Train Loss: 2307921152.0000, Validation Loss: 1990540160.0000\n","Epoch 2440/3000, Train Loss: 2217182464.0000, Validation Loss: 1976397824.0000\n","Epoch 2460/3000, Train Loss: 2375582464.0000, Validation Loss: 1989120640.0000\n","Epoch 2480/3000, Train Loss: 2339851776.0000, Validation Loss: 1991350400.0000\n","Epoch 2500/3000, Train Loss: 2195403008.0000, Validation Loss: 1988621696.0000\n","Epoch 2520/3000, Train Loss: 2355149568.0000, Validation Loss: 1985802752.0000\n","Epoch 2540/3000, Train Loss: 2319047424.0000, Validation Loss: 2001234944.0000\n","Epoch 2560/3000, Train Loss: 2247842560.0000, Validation Loss: 2001864448.0000\n","Epoch 2580/3000, Train Loss: 2286459648.0000, Validation Loss: 1994510080.0000\n","Epoch 2600/3000, Train Loss: 2234972928.0000, Validation Loss: 1993290240.0000\n","Epoch 2620/3000, Train Loss: 2340468992.0000, Validation Loss: 2005541376.0000\n","Epoch 2640/3000, Train Loss: 2201338880.0000, Validation Loss: 2002013952.0000\n","Epoch 2660/3000, Train Loss: 2121979648.0000, Validation Loss: 1981671936.0000\n","Epoch 2680/3000, Train Loss: 2228997632.0000, Validation Loss: 1984219520.0000\n","Epoch 2700/3000, Train Loss: 2224340736.0000, Validation Loss: 1973679872.0000\n","Epoch 2720/3000, Train Loss: 2301945344.0000, Validation Loss: 1988088192.0000\n","Epoch 2740/3000, Train Loss: 2105482368.0000, Validation Loss: 1983572992.0000\n","Epoch 2760/3000, Train Loss: 2477021952.0000, Validation Loss: 1992500480.0000\n","Epoch 2780/3000, Train Loss: 2193371904.0000, Validation Loss: 1994457984.0000\n","Epoch 2800/3000, Train Loss: 2168771584.0000, Validation Loss: 1996940288.0000\n","Epoch 2820/3000, Train Loss: 2358241536.0000, Validation Loss: 1992393856.0000\n","Epoch 2840/3000, Train Loss: 2087117696.0000, Validation Loss: 1978550912.0000\n","Epoch 2860/3000, Train Loss: 2073026560.0000, Validation Loss: 1987731072.0000\n","Epoch 2880/3000, Train Loss: 2247441152.0000, Validation Loss: 1975568768.0000\n","Epoch 2900/3000, Train Loss: 2376865280.0000, Validation Loss: 1973326848.0000\n","Epoch 2920/3000, Train Loss: 2235407872.0000, Validation Loss: 1981969536.0000\n","Epoch 2940/3000, Train Loss: 2339633152.0000, Validation Loss: 1981121024.0000\n","Epoch 2960/3000, Train Loss: 2049509632.0000, Validation Loss: 1977536256.0000\n","Epoch 2980/3000, Train Loss: 2269377536.0000, Validation Loss: 1994769280.0000\n","Validation RMSE: 44621.66\n","<class 'numpy.ndarray'>\n","(146, 1)\n","[[134369.72]\n"," [307358.06]\n"," [118869.3 ]\n"," [172219.05]\n"," [259012.22]]\n","[250000 187100 133900  67000 137500]\n"]}]},{"cell_type":"code","source":["# 4 - Predictions on the Validation Set:\n","# Once the base models have been trained,\n","# they are used to make predictions on the validation set."],"metadata":{"id":"2A74bSkTam_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5 - Developing a Meta Model:\n","# The next stage is to develop a meta-model, also known as a meta learner,\n","# which will take the predictions of the underlying models as input\n","# and make the final prediction. Any algorithm, such as linear regression,\n","# logistic regression, or even a neural network, can be used to create this model.\n","\n","print(model0_pred[:5])\n","print(model1_pred[:5])\n","X_meta = np.concatenate((model0_pred, model1_pred), axis=1)\n","print(X_meta[:5])\n","print(X_meta.shape)\n","\n","X_train_meta = torch.tensor(X_meta, dtype=torch.float32)\n","# X_test_meta = torch.tensor(X_test, dtype=torch.float32)\n","y_train_meta = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n","# y_test_meta = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n","\n","# Define the model\n","class MetaModel(nn.Module):\n","    def __init__(self, input_size):\n","        super(MetaModel, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(input_size, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","# Initialize the model\n","input_size = X_train_meta.shape[1]\n","model_meta = MetaModel(input_size)\n","\n","# Define loss and optimizer\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model_meta.parameters(), lr=0.001)\n","\n","# Training loop\n","epochs = 3000\n","for epoch in range(epochs):\n","    model_meta.train()\n","    optimizer.zero_grad()\n","    predictions = model_meta(X_train_meta)\n","    loss = criterion(predictions, y_train_meta)\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Validation\n","    # model.eval()\n","    # with torch.no_grad():\n","    #     val_predictions = model(X_test_meta)\n","    #     val_loss = criterion(val_predictions, y_test_meta)\n","\n","    # if epoch % 20 == 0:\n","    #     print(f\"Epoch {epoch}/{epochs}, Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n","\n","# Evaluate on validation set\n","model_meta.eval()\n","with torch.no_grad():\n","    predictions = model_meta(X_train_meta)\n","    rmse = np.sqrt(mean_squared_error(y_train_meta.numpy(), predictions.numpy()))\n","print(f\"Train RMSE: {rmse:.2f}\")\n","\n","predictions = predictions.numpy()\n","print(type(predictions))\n","print(predictions.shape)\n","print(predictions[:5])\n","print(y_val[:5])\n","# y_pred = np.expand_dims(val_predictions, axis=1)\n","# print(val_predictions[:5])\n","model1_pred = val_predictions"],"metadata":{"id":"EnybqR-XaxJj","executionInfo":{"status":"ok","timestamp":1735485111524,"user_tz":-60,"elapsed":9930,"user":{"displayName":"L","userId":"12971862049250805533"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0881a0c0-ecdb-4bcd-ec9d-5473429078ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[142370.94]\n"," [312801.12]\n"," [114955.88]\n"," [162205.28]\n"," [317470.34]]\n","[[134369.72]\n"," [307358.06]\n"," [118869.3 ]\n"," [172219.05]\n"," [259012.22]]\n","[[142370.94 134369.72]\n"," [312801.12 307358.06]\n"," [114955.88 118869.3 ]\n"," [162205.28 172219.05]\n"," [317470.34 259012.22]]\n","(146, 2)\n","Train RMSE: 27023.91\n","<class 'numpy.ndarray'>\n","(146, 1)\n","[[144080.58]\n"," [315481.28]\n"," [115893.22]\n"," [164064.45]\n"," [326956.28]]\n","[154500 325000 115000 159000 315500]\n"]}]},{"cell_type":"code","source":["# 6 - Training the Meta Model:\n","# The meta-model is then trained using the predictions given by\n","# the base models on the validation set. The base models’ predictions\n","# serve as features for the meta-model."],"metadata":{"id":"4b8mZWyBbN_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7 - Making Test Set Predictions:\n","# Finally, the meta-model is used to produce test set predictions.\n","# The basic models’ predictions on the test set are fed into the meta-model,\n","# which then makes the final prediction.\n","X = df_test.values\n","pred_0 = model_0.predict(X)\n","pred_0 = np.expand_dims(pred_0, axis=1)\n","pred_1 = model_1(torch.tensor(X, dtype=torch.float32))\n","pred_1 = pred_1.detach().numpy()\n","\n","print(pred_0[:5])\n","print(pred_1[:5])\n","X_meta = np.concatenate((pred_0, pred_1), axis=1)\n","print(X_meta[:5])\n","print(X_meta.shape)\n","X_meta = torch.tensor(X_meta, dtype=torch.float32)\n","predictions = model_meta(X_meta)\n","predictions = predictions.detach().numpy()\n","predictions = np.squeeze(np.asarray(predictions))\n","print(predictions[:5])\n","\n","predictions = pd.DataFrame({\n","        \"Id\": df_test_loaded[\"Id\"],\n","        \"SalePrice\": predictions\n","    })\n","\n","def make_submission(predictions):\n","    path=\"/content/drive/MyDrive/2-folder/kaggle/housing-prices-competition/submission.csv\"\n","    predictions.to_csv(path, index=False)\n","    print(f\"Submission exported to {path}\")\n","\n","make_submission(predictions)"],"metadata":{"id":"rTgQi2GwbdA0","executionInfo":{"status":"ok","timestamp":1735485111525,"user_tz":-60,"elapsed":18,"user":{"displayName":"L","userId":"12971862049250805533"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b63a6703-cf69-411b-e950-f6d88dfd540a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[131184.11]\n"," [157972.42]\n"," [182508.44]\n"," [190153.8 ]\n"," [192972.12]]\n","[[151100.02]\n"," [180509.12]\n"," [188190.72]\n"," [191802.22]\n"," [165247.31]]\n","[[131184.11 151100.02]\n"," [157972.42 180509.12]\n"," [182508.44 188190.72]\n"," [190153.8  191802.22]\n"," [192972.12 165247.31]]\n","(1459, 2)\n","[135464.66 162673.83 183950.14 191535.25 197646.39]\n","Submission exported to /content/drive/MyDrive/2-folder/kaggle/housing-prices-competition/submission.csv\n"]}]},{"cell_type":"code","source":["# 8 - Model Evaluation: The final stage is to assess\n","# the stacking ensemble’s performance. This is accomplished\n","# by comparing the stacking ensemble’s predictions to the actual values\n","# on the test set using evaluation measures such as accuracy, precision,\n","# recall, F1 score, and so on."],"metadata":{"id":"zdCDNKiEbyX0"},"execution_count":null,"outputs":[]}]}
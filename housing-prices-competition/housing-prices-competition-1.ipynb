{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1b7RV-22-Hanw1tMl0w9l_6Qa6CXeumvc","authorship_tag":"ABX9TyPKzaymdHVvRkavO16k6DlP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","!cp /content/drive/MyDrive/2-folder/kaggle/df_utils.py /content/\n","import df_utils"],"metadata":{"id":"N38m0isXjjUB","executionInfo":{"status":"ok","timestamp":1735411608894,"user_tz":-60,"elapsed":326,"user":{"displayName":"L","userId":"12971862049250805533"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# 1 - Preparing the Data:\n","# The first step is to prepare the data for modeling.\n","# This entails identifying the relevant features, cleaning the data,\n","# and dividing it into training and validation sets."],"metadata":{"id":"_ZRtPpAxX0P_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv('/content/drive/MyDrive/2-folder/kaggle/housing-prices-competition/train.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/2-folder/kaggle/housing-prices-competition/test.csv')\n","\n","num_col = len(df_train.columns)\n","# there are 81 columns\n","print(f\"num_col = {num_col}\")\n","print(df_train[\"SalePrice\"][:5])\n","\n","# drop all columns with with more than 20% of missing values\n","percent_missing = df_train.isnull().sum() * 100 / len(df_train)\n","missing_value_df = pd.DataFrame({'column_name': df_train.columns,\n","                                 'percent_missing': percent_missing})\n","missing_value_df = missing_value_df.loc[missing_value_df['percent_missing'] >= 20]\n","print(missing_value_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ls_Hf3IsjiuH","executionInfo":{"status":"ok","timestamp":1735413229066,"user_tz":-60,"elapsed":241,"user":{"displayName":"L","userId":"12971862049250805533"}},"outputId":"29840386-d85f-4d9c-fe90-9f418f1420b3"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["num_col = 81\n","0    208500\n","1    181500\n","2    223500\n","3    140000\n","4    250000\n","Name: SalePrice, dtype: int64\n","             column_name  percent_missing\n","Alley              Alley        93.767123\n","MasVnrType    MasVnrType        59.726027\n","FireplaceQu  FireplaceQu        47.260274\n","PoolQC            PoolQC        99.520548\n","Fence              Fence        80.753425\n","MiscFeature  MiscFeature        96.301370\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvNQqjRUWG0F"},"outputs":[],"source":["# 2 - Model Selection:\n","# The following step is to choose the base models\n","# that will be used in the stacking ensemble.\n","# A broad selection of models is typically chosen to guarantee\n","# that they produce different types of errors and complement one another."]},{"cell_type":"code","source":["# 3 - Training the Base Models:\n","# After selecting the base models, they are trained on the training set.\n","# To ensure diversity, each model is trained using a different algorithm\n","# or set of hyperparameters."],"metadata":{"id":"Vg2_XrC9aK6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4 - Predictions on the Validation Set:\n","# Once the base models have been trained,\n","# they are used to make predictions on the validation set."],"metadata":{"id":"2A74bSkTam_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5 - Developing a Meta Model:\n","# The next stage is to develop a meta-model, also known as a meta learner,\n","# which will take the predictions of the underlying models as input\n","# and make the final prediction. Any algorithm, such as linear regression,\n","# logistic regression, or even a neural network, can be used to create this model."],"metadata":{"id":"EnybqR-XaxJj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6 - Training the Meta Model:\n","# The meta-model is then trained using the predictions given by\n","# the base models on the validation set. The base models’ predictions\n","# serve as features for the meta-model."],"metadata":{"id":"4b8mZWyBbN_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7 - Making Test Set Predictions:\n","# Finally, the meta-model is used to produce test set predictions.\n","# The basic models’ predictions on the test set are fed into the meta-model,\n","# which then makes the final prediction."],"metadata":{"id":"rTgQi2GwbdA0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 8 - Model Evaluation: The final stage is to assess\n","# the stacking ensemble’s performance. This is accomplished\n","# by comparing the stacking ensemble’s predictions to the actual values\n","# on the test set using evaluation measures such as accuracy, precision,\n","# recall, F1 score, and so on."],"metadata":{"id":"zdCDNKiEbyX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pandas as pd\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.ensemble import RandomForestRegressor\n","# from sklearn.metrics import mean_squared_error\n","# from sklearn.preprocessing import OneHotEncoder\n","# from sklearn.impute import SimpleImputer\n","# from sklearn.compose import ColumnTransformer\n","# from sklearn.pipeline import Pipeline\n","\n","# # Load data\n","# train = pd.read_csv('train.csv')\n","# test = pd.read_csv('test.csv')\n","\n","# # Separate features and target\n","# X = train.drop(['Id', 'SalePrice'], axis=1)\n","# y = train['SalePrice']\n","\n","# # Split into training and validation sets\n","# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","# # Preprocessing for numerical and categorical data\n","# numerical_cols = [col for col in X_train.columns if X_train[col].dtype in ['int64', 'float64']]\n","# categorical_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\n","\n","# numerical_transformer = SimpleImputer(strategy='mean')\n","# categorical_transformer = Pipeline(steps=[\n","#     ('imputer', SimpleImputer(strategy='most_frequent')),\n","#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","# ])\n","\n","# preprocessor = ColumnTransformer(\n","#     transformers=[\n","#         ('num', numerical_transformer, numerical_cols),\n","#         ('cat', categorical_transformer, categorical_cols)\n","#     ])\n","\n","# # Define the model\n","# model = RandomForestRegressor(n_estimators=100, random_state=0)\n","\n","# # Create and evaluate pipeline\n","# clf = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n","# clf.fit(X_train, y_train)\n","# preds = clf.predict(X_valid)\n","# rmse = mean_squared_error(y_valid, preds, squared=False)\n","\n","# print(f'Validation RMSE: {rmse}')\n","\n","# # Prepare test data and make predictions for submission\n","# test_preds = clf.predict(test.drop(['Id'], axis=1))\n","# output = pd.DataFrame({'Id': test['Id'], 'SalePrice': test_preds})\n","# output.to_csv('submission.csv', index=False)"],"metadata":{"id":"Ox2hzbK-WcSb"},"execution_count":null,"outputs":[]}]}
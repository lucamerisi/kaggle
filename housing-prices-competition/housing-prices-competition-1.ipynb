{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMH1W0RrQ3k4+N1uAflBA5Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# 1 - Preparing the Data:\n","# The first step is to prepare the data for modeling.\n","# This entails identifying the relevant features, cleaning the data,\n","# and dividing it into training and validation sets."],"metadata":{"id":"_ZRtPpAxX0P_","executionInfo":{"status":"ok","timestamp":1735407936401,"user_tz":-60,"elapsed":262,"user":{"displayName":"L","userId":"12971862049250805533"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvNQqjRUWG0F"},"outputs":[],"source":["# 2 - Model Selection:\n","# The following step is to choose the base models\n","# that will be used in the stacking ensemble.\n","# A broad selection of models is typically chosen to guarantee\n","# that they produce different types of errors and complement one another."]},{"cell_type":"code","source":["# 3 - Training the Base Models:\n","# After selecting the base models, they are trained on the training set.\n","# To ensure diversity, each model is trained using a different algorithm\n","# or set of hyperparameters."],"metadata":{"id":"Vg2_XrC9aK6y","executionInfo":{"status":"ok","timestamp":1735408553670,"user_tz":-60,"elapsed":267,"user":{"displayName":"L","userId":"12971862049250805533"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 4 - Predictions on the Validation Set:\n","# Once the base models have been trained,\n","# they are used to make predictions on the validation set."],"metadata":{"id":"2A74bSkTam_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5 - Developing a Meta Model:\n","# The next stage is to develop a meta-model, also known as a meta learner,\n","# which will take the predictions of the underlying models as input\n","# and make the final prediction. Any algorithm, such as linear regression,\n","# logistic regression, or even a neural network, can be used to create this model."],"metadata":{"id":"EnybqR-XaxJj","executionInfo":{"status":"ok","timestamp":1735408711778,"user_tz":-60,"elapsed":230,"user":{"displayName":"L","userId":"12971862049250805533"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 6 - Training the Meta Model:\n","# The meta-model is then trained using the predictions given by\n","# the base models on the validation set. The base models’ predictions\n","# serve as features for the meta-model."],"metadata":{"id":"4b8mZWyBbN_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7 - Making Test Set Predictions:\n","# Finally, the meta-model is used to produce test set predictions.\n","# The basic models’ predictions on the test set are fed into the meta-model,\n","# which then makes the final prediction."],"metadata":{"id":"rTgQi2GwbdA0","executionInfo":{"status":"ok","timestamp":1735408913768,"user_tz":-60,"elapsed":222,"user":{"displayName":"L","userId":"12971862049250805533"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# 8 - Model Evaluation: The final stage is to assess\n","# the stacking ensemble’s performance. This is accomplished\n","# by comparing the stacking ensemble’s predictions to the actual values\n","# on the test set using evaluation measures such as accuracy, precision,\n","# recall, F1 score, and so on."],"metadata":{"id":"zdCDNKiEbyX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pandas as pd\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.ensemble import RandomForestRegressor\n","# from sklearn.metrics import mean_squared_error\n","# from sklearn.preprocessing import OneHotEncoder\n","# from sklearn.impute import SimpleImputer\n","# from sklearn.compose import ColumnTransformer\n","# from sklearn.pipeline import Pipeline\n","\n","# # Load data\n","# train = pd.read_csv('train.csv')\n","# test = pd.read_csv('test.csv')\n","\n","# # Separate features and target\n","# X = train.drop(['Id', 'SalePrice'], axis=1)\n","# y = train['SalePrice']\n","\n","# # Split into training and validation sets\n","# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","# # Preprocessing for numerical and categorical data\n","# numerical_cols = [col for col in X_train.columns if X_train[col].dtype in ['int64', 'float64']]\n","# categorical_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\n","\n","# numerical_transformer = SimpleImputer(strategy='mean')\n","# categorical_transformer = Pipeline(steps=[\n","#     ('imputer', SimpleImputer(strategy='most_frequent')),\n","#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","# ])\n","\n","# preprocessor = ColumnTransformer(\n","#     transformers=[\n","#         ('num', numerical_transformer, numerical_cols),\n","#         ('cat', categorical_transformer, categorical_cols)\n","#     ])\n","\n","# # Define the model\n","# model = RandomForestRegressor(n_estimators=100, random_state=0)\n","\n","# # Create and evaluate pipeline\n","# clf = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n","# clf.fit(X_train, y_train)\n","# preds = clf.predict(X_valid)\n","# rmse = mean_squared_error(y_valid, preds, squared=False)\n","\n","# print(f'Validation RMSE: {rmse}')\n","\n","# # Prepare test data and make predictions for submission\n","# test_preds = clf.predict(test.drop(['Id'], axis=1))\n","# output = pd.DataFrame({'Id': test['Id'], 'SalePrice': test_preds})\n","# output.to_csv('submission.csv', index=False)"],"metadata":{"id":"Ox2hzbK-WcSb","executionInfo":{"status":"ok","timestamp":1735407845792,"user_tz":-60,"elapsed":255,"user":{"displayName":"L","userId":"12971862049250805533"}}},"execution_count":1,"outputs":[]}]}